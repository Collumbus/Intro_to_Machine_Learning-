{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quiz: Programming a SVM\n",
    "# prep_terrain_data.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "import random\n",
    "\n",
    "\n",
    "def makeTerrainData(n_points=1000):\n",
    "###############################################################################\n",
    "### make the toy dataset\n",
    "    random.seed(42)\n",
    "    grade = [random.random() for ii in range(0,n_points)]\n",
    "    bumpy = [random.random() for ii in range(0,n_points)]\n",
    "    error = [random.random() for ii in range(0,n_points)]\n",
    "    y = [round(grade[ii]*bumpy[ii]+0.3+0.1*error[ii]) for ii in range(0,n_points)]\n",
    "    for ii in range(0, len(y)):\n",
    "        if grade[ii]>0.8 or bumpy[ii]>0.8:\n",
    "            y[ii] = 1.0\n",
    "\n",
    "### split into train/test sets\n",
    "    X = [[gg, ss] for gg, ss in zip(grade, bumpy)]\n",
    "    split = int(0.75*n_points)\n",
    "    X_train = X[0:split]\n",
    "    X_test  = X[split:]\n",
    "    y_train = y[0:split]\n",
    "    y_test  = y[split:]\n",
    "\n",
    "    grade_sig = [X_train[ii][0] for ii in range(0, len(X_train)) if y_train[ii]==0]\n",
    "    bumpy_sig = [X_train[ii][1] for ii in range(0, len(X_train)) if y_train[ii]==0]\n",
    "    grade_bkg = [X_train[ii][0] for ii in range(0, len(X_train)) if y_train[ii]==1]\n",
    "    bumpy_bkg = [X_train[ii][1] for ii in range(0, len(X_train)) if y_train[ii]==1]\n",
    "\n",
    "#    training_data = {\"fast\":{\"grade\":grade_sig, \"bumpiness\":bumpy_sig}\n",
    "#            , \"slow\":{\"grade\":grade_bkg, \"bumpiness\":bumpy_bkg}}\n",
    "\n",
    "\n",
    "    grade_sig = [X_test[ii][0] for ii in range(0, len(X_test)) if y_test[ii]==0]\n",
    "    bumpy_sig = [X_test[ii][1] for ii in range(0, len(X_test)) if y_test[ii]==0]\n",
    "    grade_bkg = [X_test[ii][0] for ii in range(0, len(X_test)) if y_test[ii]==1]\n",
    "    bumpy_bkg = [X_test[ii][1] for ii in range(0, len(X_test)) if y_test[ii]==1]\n",
    "\n",
    "    test_data = {\"fast\":{\"grade\":grade_sig, \"bumpiness\":bumpy_sig}\n",
    "            , \"slow\":{\"grade\":grade_bkg, \"bumpiness\":bumpy_bkg}}\n",
    "\n",
    "    return X_train, y_train, X_test, y_test\n",
    "#    return training_data, test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# class_vis.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "\n",
    "#from udacityplots import *\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import matplotlib\n",
    "#matplotlib.use('agg')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pylab as pl\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "def prettyPicture(clf, X_test, y_test):\n",
    "    x_min = 0.0; x_max = 1.0\n",
    "    y_min = 0.0; y_max = 1.0\n",
    "\n",
    "    # Plot the decision boundary. For that, we will assign a color to each\n",
    "    # point in the mesh [x_min, m_max]x[y_min, y_max].\n",
    "    h = .01  # step size in the mesh\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "    # Put the result into a color plot\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.xlim(xx.min(), xx.max())\n",
    "    plt.ylim(yy.min(), yy.max())\n",
    "\n",
    "    plt.pcolormesh(xx, yy, Z, cmap=pl.cm.seismic)\n",
    "\n",
    "    # Plot also the test points\n",
    "    grade_sig = [X_test[ii][0] for ii in range(0, len(X_test)) if y_test[ii]==0]\n",
    "    bumpy_sig = [X_test[ii][1] for ii in range(0, len(X_test)) if y_test[ii]==0]\n",
    "    grade_bkg = [X_test[ii][0] for ii in range(0, len(X_test)) if y_test[ii]==1]\n",
    "    bumpy_bkg = [X_test[ii][1] for ii in range(0, len(X_test)) if y_test[ii]==1]\n",
    "\n",
    "    plt.scatter(grade_sig, bumpy_sig, color = \"b\", label=\"fast\")\n",
    "    plt.scatter(grade_bkg, bumpy_bkg, color = \"r\", label=\"slow\")\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"bumpiness\")\n",
    "    plt.ylabel(\"grade\")\n",
    "\n",
    "    plt.savefig(\"test.png\")\n",
    "\n",
    "\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# studentMain.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ClassifyNB accuracy:---------->92.00 %\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import numpy as np\n",
    "import pylab as pl\n",
    "\n",
    "\n",
    "features_train, labels_train, features_test, labels_test = makeTerrainData()\n",
    "\n",
    "\n",
    "########################## SVM #################################\n",
    "### we handle the import statement and SVC creation for you here\n",
    "from sklearn.svm import SVC\n",
    "clf = SVC(kernel=\"linear\")\n",
    "\n",
    "\n",
    "#### now your job is to fit the classifier\n",
    "#### using the training features/labels, and to\n",
    "#### make a set of predictions on the test data\n",
    "\n",
    "clf.fit(features_train, labels_train)\n",
    "\n",
    "\n",
    "#### store your predictions in a list named pred\n",
    "\n",
    "pred = clf.predict(features_test)\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "acc = accuracy_score(pred, labels_test)\n",
    "\n",
    "def submitAccuracy():\n",
    "    return acc\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "print ('ClassifyNB accuracy:---------->%.2f %%' % (accuracy_score(labels_test, pred)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mini-project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# email_preproccess.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "\n",
    "import pickle\n",
    "import cPickle\n",
    "import numpy\n",
    "\n",
    "from sklearn import cross_validation\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import SelectPercentile, f_classif\n",
    "\n",
    "\n",
    "\n",
    "def preprocess(words_file = \"../ud120-projects/tools/word_data.pkl\", authors_file=\"../ud120-projects/tools/email_authors.pkl\"):\n",
    "    \"\"\" \n",
    "        this function takes a pre-made list of email texts (by default word_data.pkl)\n",
    "        and the corresponding authors (by default email_authors.pkl) and performs\n",
    "        a number of preprocessing steps:\n",
    "            -- splits into training/testing sets (10% testing)\n",
    "            -- vectorizes into tfidf matrix\n",
    "            -- selects/keeps most helpful features\n",
    "\n",
    "        after this, the feaures and labels are put into numpy arrays, which play nice with sklearn functions\n",
    "\n",
    "        4 objects are returned:\n",
    "            -- training/testing features\n",
    "            -- training/testing labels\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    ### the words (features) and authors (labels), already largely preprocessed\n",
    "    ### this preprocessing will be repeated in the text learning mini-project\n",
    "    authors_file_handler = open(authors_file, \"r\")\n",
    "    authors = pickle.load(authors_file_handler)\n",
    "    authors_file_handler.close()\n",
    "\n",
    "    words_file_handler = open(words_file, \"r\")\n",
    "    word_data = cPickle.load(words_file_handler)\n",
    "    words_file_handler.close()\n",
    "\n",
    "    ### test_size is the percentage of events assigned to the test set\n",
    "    ### (remainder go into training)\n",
    "    features_train, features_test, labels_train, labels_test = cross_validation.train_test_split(word_data, authors, test_size=0.1, random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "    ### text vectorization--go from strings to lists of numbers\n",
    "    vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.5,\n",
    "                                 stop_words='english')\n",
    "    features_train_transformed = vectorizer.fit_transform(features_train)\n",
    "    features_test_transformed  = vectorizer.transform(features_test)\n",
    "\n",
    "\n",
    "\n",
    "    ### feature selection, because text is super high dimensional and \n",
    "    ### can be really computationally chewy as a result\n",
    "    selector = SelectPercentile(f_classif, percentile=10)\n",
    "    selector.fit(features_train_transformed, labels_train)\n",
    "    features_train_transformed = selector.transform(features_train_transformed).toarray()\n",
    "    features_test_transformed  = selector.transform(features_test_transformed).toarray()\n",
    "\n",
    "    ### info on the data\n",
    "    print \"no. of Chris training emails:\", sum(labels_train)\n",
    "    print \"no. of Sara training emails:\", len(labels_train)-sum(labels_train)\n",
    "    \n",
    "    return features_train_transformed, features_test_transformed, labels_train, labels_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# svm_author_id.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no. of Chris training emails: 7936\n",
      "no. of Sara training emails: 7884\n",
      "\n",
      "Are he SVM training and predicting times faster or slower than Naive Bayes?\n",
      "The fit time is: 84.44 s\n",
      "The training time is: 8.22 s\n",
      "\n",
      "What is the accuracy of your author identification SVM?\n",
      "The accuracy is: 0.99\n",
      "The class of index 10, 26, 50 is: 1, 0, 1\n",
      "The nunber of predicted Chris (1) class is 877 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/python\n",
    "\n",
    "\"\"\"\n",
    "    This is the code to accompany the Lesson 2 (SVM) mini-project.\n",
    "\n",
    "    Use a SVM to identify emails from the Enron corpus by their authors:\n",
    "    Sara has label 0\n",
    "    Chris has label 1\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "from time import time\n",
    "\n",
    "\n",
    "\n",
    "### features_train and features_test are the features for the training\n",
    "### and testing datasets, respectively\n",
    "### labels_train and labels_test are the corresponding item labels\n",
    "features_train, features_test, labels_train, labels_test = preprocess()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#########################################################\n",
    "### your code goes here ###\n",
    "\n",
    "def classify(features_train, labels_train, features_test, labels_test):\n",
    "    \"\"\" compute the accuracy of your SVM classifier \"\"\"\n",
    "    ### import the sklearn module for SVM\n",
    "\n",
    "    from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "    #features_train = features_train[:len(features_train)/100]\n",
    "    #labels_train = labels_train[:len(labels_train)/100]\n",
    "\n",
    "    #clf = SVC(kernel=\"linear\")\n",
    "    clf = SVC(kernel=\"rbf\", C=10000)\n",
    "    t0 = time()\n",
    "    clf.fit(features_train, labels_train)\n",
    "    fit_t = round(time()-t0, 3)\n",
    "\n",
    "    t0 = time()\n",
    "    pred = clf.predict(features_test)\n",
    "    pred_t = round(time()-t0, 3)\n",
    "\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    acc = accuracy_score(pred, labels_test)\n",
    "    return [fit_t, pred_t, acc, pred]\n",
    "\n",
    "print ''\n",
    "results = classify(features_train, labels_train, features_test, labels_test)\n",
    "print 'Are he SVM training and predicting times faster or slower than Naive Bayes?'\n",
    "print ('The fit time is: %.2f s' % results[0])\n",
    "print ('The training time is: %.2f s' % results[1])\n",
    "print ''\n",
    "print 'What is the accuracy of your author identification SVM?'\n",
    "print ('The accuracy is: %.2f' % results[2])\n",
    "\n",
    "l = results[3]\n",
    "\n",
    "print ('The class of index 10, 26, 50 is: %i, %i, %i' % (l[10], l[26], l[50]))\n",
    "\n",
    "from collections import Counter\n",
    "lll = Counter(l)\n",
    "print ('The nunber of predicted Chris (1) class is %i ' % lll[1])\n",
    "print ''\n",
    "\n",
    "\n",
    "#########################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
